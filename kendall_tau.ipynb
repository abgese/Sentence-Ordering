{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from OrderingModels import *\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = ''# Fill out location of sentences and permutations file created by data preparation notebooks\n",
    "modelFileName = ''# Fill out name of trained model file\n",
    "split_at = 30\n",
    "batch_size = 100\n",
    "max_seq_len = 10\n",
    "max_sent_len = 40\n",
    "max_num_word = 2000\n",
    "EMBEDDING_DIM = 300\n",
    "usePointerBasedLSTM = False # Work in progress\n",
    "useWordLevelEmbeddings = False\n",
    "usePretrainedWordEmbeddings = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulateKendallTau(y_true, y_pred):\n",
    "#     y_true = K.eval(y_true)\n",
    "#     y_pred = K.eval(y_pred)\n",
    "    \n",
    "    y_true_indexed = (np.argmax(y_true, axis=-1) + 1)\n",
    "    y_pred_indexed = (np.argmax(y_pred, axis=-1) + 1)\n",
    "    \n",
    "    \n",
    "    corrected_y_pred_indexed = (y_true_indexed != max_seq_len) * y_pred_indexed\n",
    "    corrected_y_pred_indexed[corrected_y_pred_indexed == 0] = max_seq_len\n",
    "    \n",
    "    kendal_tau_list = []\n",
    "    for i in range(y_true_indexed.shape[0]):\n",
    "#         print(stats.kendalltau(y_true_indexed[i],corrected_y_pred_indexed[i])[0])\n",
    "        kendal_tau_list.append(stats.kendalltau(y_true_indexed[i],corrected_y_pred_indexed[i])[0])\n",
    "    \n",
    "    return np.mean(kendal_tau_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.loadtxt(data_folder + 'permutations.txt', delimiter='\\t', dtype=int)\n",
    "\n",
    "with open(data_folder + 'sentences.txt', encoding='utf8') as f:\n",
    "    sentences = f.readlines()\n",
    "\n",
    "inc = 1\n",
    "if(usePointerBasedLSTM):\n",
    "    # pointer lstm assumes output decision space has equal length to input.\n",
    "    inc = 0 \n",
    "\n",
    "YY = []\n",
    "for y_ in y:\n",
    "    yy = []\n",
    "    dummyVec = np.array([ 0 for i in range(max_seq_len + inc)])\n",
    "    for yy_ in y_:\n",
    "        dummyVec[yy_] = 1\n",
    "        yy.append(np.copy(dummyVec))\n",
    "        dummyVec[yy_] = 0\n",
    "    YY.append(yy)\n",
    "\n",
    "YY = np.asarray(YY)\n",
    "\n",
    "if(useWordLevelEmbeddings):\n",
    "    tokenizer_file_extra = ''\n",
    "    embeddings_index = {}\n",
    "    with open('./Pretrained/glove.6B/glove.6B.300d.txt', encoding=\"utf8\") as f:\n",
    "        for line in f.readlines():\n",
    "            word, coefs = line.split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, 'f', sep=' ')\n",
    "            embeddings_index[word] = coefs\n",
    "    \n",
    "    if(usePretrainedWordEmbeddings):\n",
    "        tokenizer_file_extra = '_pretrained'\n",
    "        \n",
    "    with open('tokenizer' + tokenizer_file_extra + '.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "    X = []\n",
    "    for line in sentences:\n",
    "        lineSents = line.split('\\t')\n",
    "        xx = np.zeros((max_seq_len, max_sent_len))\n",
    "        j = 0\n",
    "        for ls in lineSents:\n",
    "            ls_vec = tokenizer.texts_to_sequences([ls])\n",
    "            ls_vec = pad_sequences(ls_vec, maxlen=max_sent_len)\n",
    "            xx[j] = np.copy(ls_vec[0])\n",
    "            j+= 1\n",
    "        X.append(np.copy(xx))\n",
    "\n",
    "    X = np.asarray(X)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    num_words = min(max_num_word, len(word_index) + 1)\n",
    "\n",
    "    if(usePretrainedWordEmbeddings):\n",
    "        embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "        for word, i in word_index.items():\n",
    "            if i >= max_num_word:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        embedding_layer = Embedding(num_words, EMBEDDING_DIM, input_length=max_sent_len, embeddings_initializer=Constant(embedding_matrix), trainable=False)\n",
    "\n",
    "    else:\n",
    "        embedding_layer = Embedding(num_words, EMBEDDING_DIM, input_length=max_sent_len, trainable=True)\n",
    "\n",
    "else:\n",
    "    X = []\n",
    "    for line in sentences:\n",
    "        lineSents = line.split('\\t')\n",
    "        xx = [[\" \"] for i in range(max_seq_len)]\n",
    "        j = 0\n",
    "        for ls in lineSents:\n",
    "            xx[j][0] = ls\n",
    "            j += 1\n",
    "        X.append(xx)\n",
    "    \n",
    "    X = np.asarray(X)\n",
    "    \n",
    "x_test = X[split_at:]\n",
    "\n",
    "y_test = YY[split_at:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLstmBased(max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    K.set_session(session)\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    session.run(tf.tables_initializer())\n",
    "    model.load_weights('./Models/' + modelFileName)\n",
    "    y_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_onehot = []\n",
    "for y_ in y_pred:\n",
    "    yy = []\n",
    "    dummyVec = np.array([ 0 for i in range(max_seq_len + inc)])\n",
    "    mask = np.ones(max_seq_len + inc)\n",
    "    for yy_ in y_:\n",
    "        yy_ = yy_*(mask) # ensure only unique positions are selected\n",
    "        dummyVec[np.argmax(yy_)] = 1\n",
    "        mask[np.argmax(yy_)] = 0 # remove alreaddy selected positions in subsequent selections\n",
    "        mask[-1] = 1 # Filler position can be selected at anytime\n",
    "        yy.append(np.copy(dummyVec))\n",
    "        dummyVec[np.argmax(yy_)] = 0\n",
    "    y_pred_onehot.append(yy)\n",
    "\n",
    "y_pred_onehot = np.asarray(y_pred_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calulateKendallTau(y_test,y_pred_onehot)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
